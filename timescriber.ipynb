{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aeURYyrv28yW"
      },
      "outputs": [],
      "source": [
        "!pip install -q whisper-timestamped onnxruntime torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3oxBpbL3ogu",
        "outputId": "4b8ac978-7d5c-4f15-af8e-b1ef3b8995d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, datetime, torch\n",
        "import whisper_timestamped as whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VHwAx1Pf5Vif",
        "outputId": "c65c3db8-66d0-44bb-947a-e4e7e2903f52"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing the dtw module. When using in academic works please cite:\n",
            "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
            "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# notebook options\n",
        "\n",
        "recording_path = \"/content/drive/MyDrive/Recording/input\"\n",
        "output_file = \"transcript.txt\"\n",
        "# possible sizes are tiny, base, small, medium and large\n",
        "# there are also english-only models with tiny.en, medium.en and such\n",
        "whisper_model = \"tiny\" # change this to medium for it to be good, I'm just testing"
      ],
      "metadata": {
        "id": "rHsgNiCxCdg2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeScribe:\n",
        "    def __init__(self, recording_folder: str,\n",
        "                 output_name: str = \"transcript.txt\",\n",
        "                 model_size: str = \"medium\"):\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "            print(\"Using GPU\")\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "            print(\"Using CPU\")\n",
        "\n",
        "        self.recording_folder = recording_folder\n",
        "        self.output_name = output_name\n",
        "        self.model = whisper.load_model(model_size, self.device)\n",
        "        self.name_pattern = r\"-(\\w+)_\"\n",
        "\n",
        "    def transcribe_folder(self) -> None:\n",
        "\n",
        "        # get all files in input path\n",
        "        files = os.listdir(self.recording_folder)\n",
        "\n",
        "        # speaker_segments = dict()\n",
        "        transcript = list()\n",
        "\n",
        "        # iterate through files\n",
        "        for file in files:\n",
        "            # if file is an audio file, transcribe it\n",
        "            if file.endswith(\".wav\") or file.endswith(\".flac\"):\n",
        "                audio = whisper.load_audio(os.path.join(self.recording_folder, file))\n",
        "                result = whisper.transcribe(self.model, audio, language=\"en\", vad=True)\n",
        "                for segment in result[\"segments\"]:\n",
        "                    name_match = re.search(self.name_pattern, file)\n",
        "                    segment_time = float(segment[\"start\"])\n",
        "                    if name_match:\n",
        "                      transcript.append((segment_time,\n",
        "                                         name_match.group(1),\n",
        "                                         segment[\"text\"],\n",
        "                                         str(datetime.timedelta(seconds=round(segment_time)))))\n",
        "                    else:\n",
        "                      transcript.append((float(segment[\"start\"]),\n",
        "                                         file, segment[\"text\"],\n",
        "                                         str(datetime.timedelta(seconds=round(segment_time)))))\n",
        "\n",
        "            # otherwise, ignore it\n",
        "            else:\n",
        "                print(f\"Skipping {file}\")\n",
        "\n",
        "        transcript.sort(key=lambda a: a[0])\n",
        "        # write transcript to file in sorted order with timestamps\n",
        "        with open(f\"{self.recording_folder}/{self.output_name}\", \"w\") as f:\n",
        "            for segment in transcript:\n",
        "                f.write(f\"[{segment[3]}] {segment[1]}: {segment[2]}\\n\")"
      ],
      "metadata": {
        "id": "AelD5mZp5W6i"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timescribe = TimeScribe(recording_folder=recording_path,\n",
        "                        output_name=output_file,\n",
        "                        model_size=whisper_model)\n",
        "timescribe.transcribe_folder()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKHvdgDy7P2f",
        "outputId": "16e7e7bd-8a34-49b2-9038-94a477c3e9d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 72.1M/72.1M [00:00<00:00, 103MiB/s]\n",
            "100%|██████████| 109973/109973 [00:45<00:00, 2443.23frames/s]\n",
            "100%|██████████| 95584/95584 [00:44<00:00, 2129.68frames/s]\n",
            "100%|██████████| 7483/7483 [00:01<00:00, 4260.62frames/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping info.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 58302/58302 [00:19<00:00, 3031.87frames/s]\n",
            "100%|██████████| 6474/6474 [00:02<00:00, 2348.43frames/s]\n",
            "100%|██████████| 7945/7945 [00:02<00:00, 3494.76frames/s]\n",
            "100%|██████████| 29972/29972 [00:11<00:00, 2525.87frames/s]\n",
            " 99%|█████████▊| 21592/21902 [00:06<00:00, 3295.16frames/s]\n",
            "100%|██████████| 3019/3019 [00:01<00:00, 2208.92frames/s]\n",
            "100%|██████████| 10811/10811 [00:03<00:00, 2739.87frames/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping raw.dat\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}